{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torchvision.models as  models\n",
    "from torch.utils.data import DataLoader\n",
    "from src.dataset import PascalVOC_Dataset\n",
    "import torch.optim as optim\n",
    "from src.utils import encode_labels, plot_history\n",
    "import os\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import src.utils as utils \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=[0.457342265910642, 0.4387686270106377, 0.4073427106250871]\n",
    "std=[0.26753769276329037, 0.2638145880487105, 0.2776826934044154]\n",
    "    \n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "    \n",
    "transformations = transforms.Compose([transforms.Resize((224, 224)),\n",
    "#                                      transforms.RandomChoice([\n",
    "#                                              transforms.CenterCrop(300),\n",
    "#                                              transforms.RandomResizedCrop(300, scale=(0.80, 1.0)),\n",
    "#                                              ]),                                      \n",
    "                                      transforms.RandomChoice([\n",
    "                                          transforms.ColorJitter(brightness=(0.80, 1.20)),\n",
    "                                          transforms.RandomGrayscale(p = 0.25)\n",
    "                                          ]),\n",
    "                                      transforms.RandomHorizontalFlip(p = 0.25),\n",
    "                                      transforms.RandomRotation(25),\n",
    "                                      transforms.ToTensor(), \n",
    "                                      transforms.Normalize(mean = mean, std = std),\n",
    "                                      ])\n",
    "        \n",
    "transformations_valid = transforms.Compose([transforms.Resize((224,224)), \n",
    "                                          transforms.ToTensor(), \n",
    "                                           transforms.Normalize(mean = mean, std = std),\n",
    "                                          ])\n",
    "data_dir='./VOCdevkit-1'\n",
    "# Create train dataloader\n",
    "dataset_train = PascalVOC_Dataset(data_dir,\n",
    "                                      year='2007', \n",
    "                                      image_set='train', \n",
    "                                      download=False, \n",
    "                                      transform=transformations, \n",
    "                                      target_transform=encode_labels)\n",
    "    \n",
    "train_loader = DataLoader(dataset_train, batch_size=5, num_workers=0, shuffle=True)\n",
    "dataset_valid = PascalVOC_Dataset(data_dir,\n",
    "                                      year='2007', \n",
    "                                      image_set='val', \n",
    "                                      download=False, \n",
    "                                      transform=transformations_valid, \n",
    "                                      target_transform=encode_labels)\n",
    "    \n",
    "valid_loader = DataLoader(dataset_valid, batch_size=5, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50 = models.resnet50(pretrained=True).to(device)\n",
    "    \n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False   \n",
    "\n",
    "resnet50.fc = nn.Sequential(nn.Linear(2048, 128),\n",
    "               nn.ReLU(inplace=True),\n",
    "               nn.Linear(128, 20)).to(device)\n",
    "resnet50.load_state_dict(torch.load('models/Resnet50.h5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet = models.alexnet(pretrained=True).to(device)\n",
    "    \n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False   \n",
    "\n",
    "\n",
    "\n",
    "for param in alexnet.parameters():\n",
    "  param.requires_grad = False\n",
    "alexnet.classifier[4]=nn.Linear(4096,128)\n",
    "alexnet.classifier[6]=nn.Linear(128,20).to(device)\n",
    "alexnet.load_state_dict(torch.load('models/AlexNet.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inceptionv3 = models.inception_v3(pretrained=True).to(device)\n",
    "    \n",
    "for param in inceptionv3.parameters():\n",
    "    param.requires_grad = False   \n",
    "\n",
    "inceptionv3.aux_logits = False\n",
    "\n",
    "num_ftrs = inceptionv3.fc.in_features\n",
    "inceptionv3.fc = torch.nn.Sequential(\n",
    "    nn.Linear(num_ftrs,128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(128,20)\n",
    ")\n",
    "inceptionv3.load_state_dict(torch.load('models/Inception_V3.h5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effecientnet=models.efficientnet_b0(pretrained=True).to(device)\n",
    "for params in effecientnet.classifier.parameters():\n",
    "    params.requires_grad=False\n",
    "effecientnet.classifier=nn.Sequential(\n",
    "    nn.Linear(1280,128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(128,20)\n",
    "\n",
    ")\n",
    "effecientnet.load_state_dict(torch.load('models/EffecientNet.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobilenet=models.mobilenet_v2(pretrained=True).to(device)\n",
    "for param in mobilenet.classifier.parameters():\n",
    "    param.requires_grad=False\n",
    "mobilenet.classifier=nn.Sequential(\n",
    "    nn.Linear(1280,128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(128,20)\n",
    "\n",
    ")\n",
    "mobilenet.load_state_dict(torch.load('models/MobileNet.h5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_pred_prob=[]\n",
    "resnet50.train(False)\n",
    "for idx,(data,target) in enumerate(valid_loader):\n",
    "    output=resnet50(data)\n",
    "    probs=torch.nn.functional.softmax(output,1).cpu()\n",
    "    for prob in probs:\n",
    "        resnet50_pred_prob.append(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(0,len(resnet50_pred_prob)):\n",
    "    resnet50_pred_prob[index]=resnet50_pred_prob[index].cpu().detach().numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet_pred_prob=[]\n",
    "alexnet.train(False)\n",
    "for idx,(data,target) in enumerate(valid_loader):\n",
    "    output=alexnet(data)\n",
    "    probs=torch.nn.functional.softmax(output,1).cpu()\n",
    "    for prob in probs:\n",
    "        alexnet_pred_prob.append(prob)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(0,len(alexnet_pred_prob)):\n",
    "    alexnet_pred_prob[index]=alexnet_pred_prob[index].cpu().detach().numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet_pred_prob=np.array(alexnet_pred_prob)\n",
    "resnet50_pred_prob=np.array(resnet50_pred_prob)\n",
    "np.savez(\"resnet50_pred_prob.npz\",resnet50_pred_prob)\n",
    "np.savez(\"alexnet_pred_prob.npz\",alexnet_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "effecientnet_pred_prob=[]\n",
    "effecientnet.train(False)\n",
    "with torch.no_grad():\n",
    "  for idx,(data,target) in enumerate(valid_loader):\n",
    "     \n",
    "        output=effecientnet(data)\n",
    "        probs=torch.nn.functional.softmax(output,1).cpu()\n",
    "        for prob in probs:\n",
    "          effecientnet_pred_prob.append(prob)\n",
    "        del data, target, output\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index  in range(0,len(effecientnet_pred_prob)):\n",
    "    effecientnet_pred_prob[index]=effecientnet_pred_prob[index].cpu().detach().numpy()\n",
    "\n",
    "effecientnet_pred_prob=np.array(effecientnet_pred_prob)\n",
    "np.savez(\"effecientnet_pred_prob.npz\",effecientnet_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_pred_prob=[]\n",
    "mobilenet.train(False)\n",
    "with torch.no_grad():\n",
    "  for idx,(data,target) in enumerate(valid_loader):\n",
    "     \n",
    "        output=mobilenet(data)\n",
    "        probs=torch.nn.functional.softmax(output,1).cpu()\n",
    "        for prob in probs:\n",
    "          mobilenet_pred_prob.append(prob)\n",
    "        del data, target, output\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index  in range(0,len(mobilenet_pred_prob)):\n",
    "    mobilenet_pred_prob[index]=mobilenet_pred_prob[index].cpu().detach().numpy()\n",
    "\n",
    "mobilenet_pred_prob=np.array(mobilenet_pred_prob)\n",
    "np.savez(\"mobilenet_pred_prob.npz\",mobilenet_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=[0.457342265910642, 0.4387686270106377, 0.4073427106250871]\n",
    "std=[0.26753769276329037, 0.2638145880487105, 0.2776826934044154]\n",
    "    \n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "    \n",
    "transformations = transforms.Compose([transforms.Resize((299, 299)),\n",
    "#                                      transforms.RandomChoice([\n",
    "#                                              transforms.CenterCrop(300),\n",
    "#                                              transforms.RandomResizedCrop(300, scale=(0.80, 1.0)),\n",
    "#                                              ]),                                      \n",
    "                                      transforms.RandomChoice([\n",
    "                                          transforms.ColorJitter(brightness=(0.80, 1.20)),\n",
    "                                          transforms.RandomGrayscale(p = 0.25)\n",
    "                                          ]),\n",
    "                                      transforms.RandomHorizontalFlip(p = 0.25),\n",
    "                                      transforms.RandomRotation(25),\n",
    "                                      transforms.ToTensor(), \n",
    "                                      transforms.Normalize(mean = mean, std = std),\n",
    "                                      ])\n",
    "        \n",
    "transformations_valid = transforms.Compose([transforms.Resize((299,299)), \n",
    "                                          transforms.ToTensor(), \n",
    "                                           transforms.Normalize(mean = mean, std = std),\n",
    "                                          ])\n",
    "data_dir='./VOCdevkit-1'\n",
    "# Create train dataloader\n",
    "dataset_train = PascalVOC_Dataset(data_dir,\n",
    "                                      year='2007', \n",
    "                                      image_set='train', \n",
    "                                      download=False, \n",
    "                                      transform=transformations, \n",
    "                                      target_transform=encode_labels)\n",
    "    \n",
    "train_loader = DataLoader(dataset_train, batch_size=5, num_workers=0, shuffle=True)\n",
    "dataset_valid = PascalVOC_Dataset(data_dir,\n",
    "                                      year='2007', \n",
    "                                      image_set='val', \n",
    "                                      download=False, \n",
    "                                      transform=transformations_valid, \n",
    "                                      target_transform=encode_labels)\n",
    "    \n",
    "valid_loader = DataLoader(dataset_valid, batch_size=5, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionv3_pred_prob=[]\n",
    "inceptionv3.train(False)\n",
    "with torch.no_grad():\n",
    "  for idx,(data,target) in enumerate(valid_loader):\n",
    "     \n",
    "        output=inceptionv3(data)\n",
    "        probs=torch.nn.functional.softmax(output,1).cpu()\n",
    "        for prob in probs:\n",
    "          inceptionv3_pred_prob.append(prob)\n",
    "        del data, target, output\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index  in range(0,len(inceptionv3_pred_prob)):\n",
    "    inceptionv3_pred_prob[index]=inceptionv3_pred_prob[index].cpu().detach().numpy()\n",
    "\n",
    "inceptionv3_pred_prob=np.array(inceptionv3_pred_prob)\n",
    "np.savez(\"inceptionv3_pred_prob.npz\",inceptionv3_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_pred_prob=np.load(\"resnet50_pred_prob.npz\")\n",
    "inceptionv3_pred_prob=np.load(\"inceptionv3_pred_prob.npz\")\n",
    "mobilenet_pred_prob=np.load(\"mobilenet_pred_prob.npz\")\n",
    "alexnet_pred_prob=np.load(\"alexnet_pred_prob.npz\")\n",
    "effecient_pred_prob=np.load(\"effecientnet_pred_prob.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_pred_prob=resnet50_pred_prob['arr_0']\n",
    "inceptionv3_pred_prob=inceptionv3_pred_prob['arr_0']\n",
    "mobilenet_pred_prob=mobilenet_pred_prob['arr_0']\n",
    "alexnet_pred_prob=alexnet_pred_prob['arr_0']\n",
    "effecient_pred_prob=effecient_pred_prob['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00698527, 0.01357401, 0.00483095, 0.00258372, 0.00213092,\n",
       "       0.00712667, 0.91883   , 0.00237778, 0.00369421, 0.00221537,\n",
       "       0.00104432, 0.00347559, 0.00116759, 0.00353224, 0.01100953,\n",
       "       0.00209423, 0.00166113, 0.00226779, 0.00512919, 0.00426955],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_pred_probs=[0]*5\n",
    "\n",
    "for index in range(0,5):\n",
    "        concat_pred_probs[index]=[]\n",
    "\n",
    "for index in range(0,2510):\n",
    "        concat_pred_probs[0].append([resnet50_pred_prob[index]])\n",
    "        concat_pred_probs[1].append([effecient_pred_prob[index]])\n",
    "        concat_pred_probs[2].append([alexnet_pred_prob[index]])\n",
    "        concat_pred_probs[3].append([inceptionv3_pred_prob[index]])\n",
    "        concat_pred_probs[4].append([mobilenet_pred_prob[index]])\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.6111665e-03, 8.8136960e-03, 7.3103065e-04, 4.4348679e-04,\n",
      "       4.4520240e-04, 1.9558598e-03, 9.7083718e-01, 1.4450429e-04,\n",
      "       6.9646246e-04, 2.0685351e-04, 1.5568979e-04, 2.8960247e-04,\n",
      "       8.4889754e-05, 4.1533695e-03, 6.4152996e-03, 4.6456611e-04,\n",
      "       1.5660979e-04, 2.0637883e-04, 8.3264115e-04, 1.3555238e-03],\n",
      "      dtype=float32)]\n",
      "[array([1.7615862e-03, 6.9713555e-03, 2.6498262e-03, 2.1270358e-03,\n",
      "       1.3870042e-02, 2.2450639e-03, 2.3323186e-03, 6.8960763e-03,\n",
      "       6.4623046e-01, 5.9025333e-04, 1.2016954e-01, 3.7123377e-03,\n",
      "       3.8586726e-04, 2.9466937e-03, 1.6492698e-02, 4.5313604e-02,\n",
      "       3.5319896e-04, 5.8152977e-02, 2.2345546e-03, 6.4564548e-02],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(concat_pred_probs[0][15])\n",
    "print(concat_pred_probs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0016111665\n",
      "0.0017615862\n"
     ]
    }
   ],
   "source": [
    "print(concat_pred_probs[0][15][0][0])\n",
    "print(concat_pred_probs[0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.76158617e-03, 6.97135553e-03, 2.64982623e-03, ...,\n",
       "         5.81529774e-02, 2.23455462e-03, 6.45645484e-02],\n",
       "        [6.98527135e-03, 1.35740135e-02, 4.83095320e-03, ...,\n",
       "         2.26778560e-03, 5.12919063e-03, 4.26955475e-03],\n",
       "        [2.50270932e-05, 3.33617878e-04, 1.49602165e-05, ...,\n",
       "         5.72031013e-06, 4.30688888e-05, 2.42965871e-05],\n",
       "        ...,\n",
       "        [5.09803696e-03, 1.53692532e-02, 1.99591159e-03, ...,\n",
       "         1.26474025e-02, 8.70989356e-03, 6.68602958e-02],\n",
       "        [9.92615297e-02, 2.90360302e-02, 1.76657643e-02, ...,\n",
       "         4.95923962e-03, 1.07968241e-01, 6.36317860e-03],\n",
       "        [2.58844258e-04, 9.64893475e-02, 4.91358223e-04, ...,\n",
       "         1.90171448e-03, 7.39267038e-04, 2.10552919e-03]],\n",
       "\n",
       "       [[1.15851872e-04, 2.79524713e-04, 5.65738650e-03, ...,\n",
       "         2.30725780e-02, 2.07096920e-04, 4.13043797e-02],\n",
       "        [1.13103772e-04, 1.28607394e-03, 5.71312942e-03, ...,\n",
       "         3.18116217e-04, 1.11396203e-03, 2.14167478e-04],\n",
       "        [2.31431404e-04, 1.22843916e-03, 4.04414925e-04, ...,\n",
       "         2.62705464e-04, 3.50397045e-06, 6.84494953e-05],\n",
       "        ...,\n",
       "        [3.92779522e-03, 1.86444959e-03, 1.36273028e-03, ...,\n",
       "         4.71219246e-04, 1.38912140e-03, 2.94005219e-03],\n",
       "        [4.57125418e-02, 2.20438946e-04, 1.56943710e-03, ...,\n",
       "         5.99531038e-03, 1.53480587e-03, 1.15015358e-03],\n",
       "        [6.53203824e-05, 2.22043470e-01, 9.27176443e-04, ...,\n",
       "         4.54911962e-03, 3.74552590e-04, 3.92813468e-03]],\n",
       "\n",
       "       [[8.03223011e-05, 3.37291509e-04, 9.89837645e-06, ...,\n",
       "         2.26596698e-01, 1.15064986e-05, 5.21880127e-02],\n",
       "        [5.13991245e-06, 1.58030679e-03, 6.95754181e-07, ...,\n",
       "         8.76806007e-06, 4.41968297e-07, 4.29970477e-08],\n",
       "        [9.12703108e-03, 2.74918731e-02, 2.76284618e-03, ...,\n",
       "         2.82231456e-04, 7.23403995e-04, 2.39835994e-04],\n",
       "        ...,\n",
       "        [9.42773244e-04, 6.72609895e-05, 2.11950595e-04, ...,\n",
       "         1.83667860e-03, 1.18987089e-04, 1.29634421e-03],\n",
       "        [1.58088244e-02, 4.92298510e-03, 5.39943925e-04, ...,\n",
       "         3.35178245e-03, 1.31578609e-01, 9.56808799e-04],\n",
       "        [1.03603164e-03, 1.64385691e-01, 6.17976300e-03, ...,\n",
       "         1.78633127e-02, 6.97180221e-05, 1.28446240e-02]],\n",
       "\n",
       "       [[6.82526036e-07, 3.58572433e-04, 5.46454521e-06, ...,\n",
       "         1.33739058e-02, 4.11110159e-06, 9.39355697e-03],\n",
       "        [1.13239980e-06, 7.22631594e-05, 5.56991927e-06, ...,\n",
       "         1.10877724e-03, 8.58905605e-06, 6.93670381e-06],\n",
       "        [5.95806632e-04, 3.07035330e-03, 4.45569446e-03, ...,\n",
       "         3.39917256e-03, 1.85730678e-05, 2.25232681e-03],\n",
       "        ...,\n",
       "        [5.61442030e-05, 6.77145317e-06, 1.14083107e-07, ...,\n",
       "         1.71515399e-06, 2.14914238e-04, 2.16196759e-06],\n",
       "        [1.42324984e-01, 7.79630209e-04, 5.82504179e-03, ...,\n",
       "         1.74824090e-03, 8.67088363e-02, 3.18620726e-03],\n",
       "        [7.51061719e-08, 1.76141307e-01, 2.94480515e-07, ...,\n",
       "         7.59430812e-04, 2.46139007e-05, 7.32455810e-05]],\n",
       "\n",
       "       [[6.76464140e-07, 1.41162200e-05, 2.07409921e-05, ...,\n",
       "         2.07577199e-02, 5.30154693e-07, 1.79238152e-02],\n",
       "        [2.38235984e-07, 2.39250094e-05, 1.09455013e-05, ...,\n",
       "         8.28718782e-07, 1.07271080e-06, 9.59748192e-08],\n",
       "        [3.78811383e-04, 2.41831746e-02, 5.22133405e-06, ...,\n",
       "         6.30968309e-04, 3.61480750e-04, 4.85933328e-04],\n",
       "        ...,\n",
       "        [1.06698321e-02, 1.24100980e-03, 2.10407816e-04, ...,\n",
       "         3.94131104e-03, 1.02925748e-02, 1.42280711e-02],\n",
       "        [8.68824031e-03, 2.09617763e-04, 1.88381318e-02, ...,\n",
       "         3.52988136e-04, 6.42570257e-02, 1.89784274e-04],\n",
       "        [3.03866487e-04, 8.88818577e-02, 1.74599463e-05, ...,\n",
       "         6.83163176e-04, 2.81714401e-05, 5.79418323e-04]]], dtype=float32)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_pred_probs[1][2500].shape\n",
    "concat_pred_probs.squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot select an axis to squeeze out which has size not equal to one",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21064\\214013393.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconcat_pred_probs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconcat_pred_probs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: cannot select an axis to squeeze out which has size not equal to one"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0016111665\n",
      "0.0017615862\n"
     ]
    }
   ],
   "source": [
    "print(concat_pred_probs[0][15][0])\n",
    "print(concat_pred_probs[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_concat_feats=[0]*2510\n",
    "for i in range(0,2510):\n",
    "     max_concat_feats[i]=[]\n",
    "index=0\n",
    "while(index < 2510):\n",
    "     for i in range(0,20):\n",
    "\n",
    "          max_concat_feats[index].append(np.max([concat_pred_probs[0][index][i],concat_pred_probs[1][index][i],concat_pred_probs[2][index][i],concat_pred_probs[3][index][i],concat_pred_probs[4][index][i]]))\n",
    "     index=index+1     \n",
    "          \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0069852713"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0016111665"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_concat_feats=[0]*2510\n",
    "for i in range(0,2510):\n",
    "     min_concat_feats[i]=[]\n",
    "index=0\n",
    "while(index < 2510):\n",
    "     for i in range(0,20):\n",
    "\n",
    "          min_concat_feats[index].append(np.min([concat_pred_probs[0][index][i],concat_pred_probs[1][index][i],concat_pred_probs[2][index][i],concat_pred_probs[3][index][i],concat_pred_probs[4][index][i]]))\n",
    "     index=index+1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_concat_feats=[0]*2510\n",
    "for i in  range(0,2510):\n",
    "     avg_concat_feats[i]=[]\n",
    "index=0\n",
    "while(index < 2510):\n",
    "     for i in range(0,20):\n",
    "\n",
    "          avg_concat_feats[index].append(np.mean([concat_pred_probs[0][index][i],concat_pred_probs[1][index][i],concat_pred_probs[2][index][i],concat_pred_probs[3][index][i],concat_pred_probs[4][index][i]]))\n",
    "     index=index+1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels=[]\n",
    "for idx,(data,targets) in enumerate(valid_loader):\n",
    "    for t in targets:\n",
    "     \n",
    "     val_labels.append(t.cpu().detach().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_categories = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
    "                     'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "                     'cow', 'diningtable', 'dog', 'horse',\n",
    "                     'motorbike', 'person', 'pottedplant',\n",
    "                     'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "label=\"VOCdevkit-1/VOCdevkit/VOC2007/ImageSets/Main\"\n",
    "\n",
    "import os\n",
    "img_Label_Dict=dict() \n",
    "temp=[]\n",
    "for object in object_categories:\n",
    "    for file in os.listdir(label):\n",
    "        temp=[]\n",
    "        name=str(object)+'_val.txt'\n",
    "        if(file==name):\n",
    "            file_obj=open(os.path.join(label,f\"{file}\"),\"r\")\n",
    "            file_names=file_obj.read()\n",
    "            lines=file_names.splitlines()\n",
    "            for line in lines:\n",
    "                content=line.split()\n",
    "                if(('-1' or '0') not in content):\n",
    "                    temp.append((content[0],content[1]))\n",
    "        if(temp):\n",
    "            img_Label_Dict[object]=temp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "correct_label_per_class=defaultdict(list)\n",
    "for c in img_Label_Dict:\n",
    "    for label in img_Label_Dict[c]:\n",
    "        correct_label_per_class[c].append(int(label[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_names_list=0\n",
    "dir='VOCdevkit-1/VOCdevkit/VOC2007/ImageSets/Main/'\n",
    "image_dir='VOCdevkit-1/VOCdevkit/VOC2007/JPEGImages/'\n",
    "for file in os.listdir(dir):\n",
    "    if(file == f\"val.txt\"):\n",
    "        #file_path=os.path.join(dir,\"\")\n",
    "        file_obj=open(os.path.join(dir,f\"{file}\"),\"r\")\n",
    "        file_names=file_obj.read()\n",
    "        file_name_list=file_names.splitlines()        \n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "predicted_probs_per_class=defaultdict(list)\n",
    "for c in object_categories:\n",
    "    for label in img_Label_Dict[c]:\n",
    "    \n",
    "        for i,s in enumerate(file_name_list):\n",
    "            if (str(label[0]) in s):\n",
    "                predicted_probs_per_class[c].append(max_concat_feats[i][counter])\n",
    "    counter=counter+1\n",
    "            \n",
    "              \n",
    "            \n",
    "           \n",
    "                        \n",
    "\n",
    "                    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9599923492762568\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_percision_per_class=[]\n",
    "for c in object_categories:\n",
    "    con_pred=[1 if x > 0.5 else 0 for x in predicted_probs_per_class[c]]\n",
    "    average_percision_per_class.append(average_precision_score(correct_label_per_class[c],con_pred))\n",
    "print(np.mean(average_percision_per_class))\n",
    "                                "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "predicted_probs_per_class=defaultdict(list)\n",
    "for c in object_categories:\n",
    "    for label in img_Label_Dict[c]:\n",
    "    \n",
    "        for i,s in enumerate(file_name_list):\n",
    "            if (str(label[0]) in s):\n",
    "                predicted_probs_per_class[c].append(avg_concat_feats[i][counter])\n",
    "    counter=counter+1\n",
    "            \n",
    "              \n",
    "            \n",
    "           \n",
    "                        \n",
    "\n",
    "                    \n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9465619850268864\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_percision_per_class=[]\n",
    "for c in object_categories:\n",
    "    con_pred=[1 if x > 0.5 else 0 for x in predicted_probs_per_class[c]]\n",
    "    average_percision_per_class.append(average_precision_score(correct_label_per_class[c],con_pred))\n",
    "print(np.mean(average_percision_per_class))\n",
    "                                "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "predicted_probs_per_class=defaultdict(list)\n",
    "for c in object_categories:\n",
    "    for label in img_Label_Dict[c]:\n",
    "    \n",
    "        for i,s in enumerate(file_name_list):\n",
    "            if (str(label[0]) in s):\n",
    "                predicted_probs_per_class[c].append(min_concat_feats[i][counter])\n",
    "    counter=counter+1\n",
    "            \n",
    "              \n",
    "            \n",
    "           \n",
    "                        \n",
    "\n",
    "                    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9370693386047739\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_percision_per_class=[]\n",
    "for c in object_categories:\n",
    "    con_pred=[1 if x > 0.5 else 0 for x in predicted_probs_per_class[c]]\n",
    "    average_percision_per_class.append(average_precision_score(correct_label_per_class[c],con_pred))\n",
    "print(np.mean(average_percision_per_class))\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9599923492762568\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_percision_per_class=[]\n",
    "for c in object_categories:\n",
    "    con_pred=[1 if x > 0.5 else 0 for x in predicted_probs_per_class[c]]\n",
    "    average_percision_per_class.append(average_precision_score(correct_label_per_class[c],con_pred))\n",
    "print(np.mean(average_percision_per_class))\n",
    "                                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
